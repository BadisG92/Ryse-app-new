#!/usr/bin/env python3
"""
Script principal pour exÃ©cuter les tests de performance - Task 10
Orchestration complÃ¨te des tests de charge et de performance
"""

import asyncio
import sys
import os
import json
import argparse
from datetime import datetime
from pathlib import Path
from typing import Dict, Any

# Ajout du chemin pour les imports
sys.path.append(os.path.dirname(os.path.abspath(__file__)))

from performance_testing import run_comprehensive_load_tests, LoadTestConfig, export_results_to_csv, generate_html_report
from test_performance_simple import run_simple_performance_tests
from performance_optimization import PerformanceOptimizer

def print_banner():
    """Affiche le banner du programme"""
    print("""
    â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
    â•‘                  ğŸš€ RYZE PERFORMANCE TESTING SUITE          â•‘
    â•‘                       Task 10 Implementation                 â•‘
    â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£
    â•‘  ğŸ“Š Load Testing  â€¢  âš¡ Performance Optimization            â•‘
    â•‘  ğŸ” Monitoring    â€¢  ğŸ“ˆ Detailed Reporting                  â•‘
    â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    """)

def parse_arguments():
    """Parse les arguments de la ligne de commande"""
    parser = argparse.ArgumentParser(description='Ryze Performance Testing Suite')
    
    parser.add_argument('--mode', 
                       choices=['simple', 'full', 'optimization-only'],
                       default='simple',
                       help='Mode de test Ã  exÃ©cuter')
    
    parser.add_argument('--url', 
                       default='http://localhost:8000',
                       help='URL de base de l\'application')
    
    parser.add_argument('--concurrent-users', 
                       type=int, 
                       default=100,
                       help='Nombre d\'utilisateurs concurrents')
    
    parser.add_argument('--total-requests', 
                       type=int, 
                       default=1000,
                       help='Nombre total de requÃªtes')
    
    parser.add_argument('--max-response-time', 
                       type=int, 
                       default=2000,
                       help='Temps de rÃ©ponse maximum acceptable (ms)')
    
    parser.add_argument('--max-error-rate', 
                       type=float, 
                       default=1.0,
                       help='Taux d\'erreur maximum acceptable (%)')
    
    parser.add_argument('--min-throughput', 
                       type=float, 
                       default=50.0,
                       help='Throughput minimum acceptable (req/s)')
    
    parser.add_argument('--output-dir', 
                       default='./performance_results',
                       help='RÃ©pertoire de sortie des rÃ©sultats')
    
    parser.add_argument('--enable-redis', 
                       action='store_true',
                       help='Activer les optimisations Redis')
    
    parser.add_argument('--verbose', '-v', 
                       action='store_true',
                       help='Mode verbeux')
    
    return parser.parse_args()

async def run_optimization_setup(enable_redis: bool = False):
    """Configure et teste les optimisations de performance"""
    print("\nğŸ”§ Configuration des optimisations de performance...")
    
    try:
        optimizer = PerformanceOptimizer()
        
        if enable_redis:
            await optimizer.initialize()
            
            # Test du statut des optimisations
            status = await optimizer.get_optimization_status()
            
            print("âœ… Status des optimisations:")
            print(f"   â€¢ Cache Redis: {'âœ… ActivÃ©' if status['cache']['enabled'] else 'âŒ DÃ©sactivÃ©'}")
            print(f"   â€¢ Monitoring: {'âœ… ActivÃ©' if status['monitoring']['enabled'] else 'âŒ DÃ©sactivÃ©'}")
            print(f"   â€¢ Pool DB: âœ… ConfigurÃ© ({status['database']['config']['pool_size']} connexions)")
            
            # VÃ©rification de performance
            perf_check = await optimizer.run_performance_check()
            print(f"\nğŸ“Š VÃ©rifications systÃ¨me:")
            for check in perf_check['checks']:
                status_icon = "âœ…" if check['status'] == 'healthy' else "âš ï¸"
                print(f"   â€¢ {check['component'].title()}: {status_icon} {check['status']}")
            
            await optimizer.cleanup()
        else:
            print("   â€¢ Cache Redis: âŒ DÃ©sactivÃ© (utiliser --enable-redis)")
            print("   â€¢ Monitoring: âœ… ActivÃ© (mode basique)")
            print("   â€¢ Pool DB: âœ… Configuration par dÃ©faut")
            
        return True
        
    except Exception as e:
        print(f"âŒ Erreur lors de la configuration: {e}")
        return False

def prepare_output_directory(output_dir: str) -> Path:
    """PrÃ©pare le rÃ©pertoire de sortie"""
    output_path = Path(output_dir)
    output_path.mkdir(exist_ok=True)
    
    # CrÃ©er un sous-rÃ©pertoire avec timestamp
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    test_run_dir = output_path / f"test_run_{timestamp}"
    test_run_dir.mkdir(exist_ok=True)
    
    return test_run_dir

async def run_simple_tests():
    """ExÃ©cute les tests simples de validation"""
    print("\nğŸ§ª ExÃ©cution des tests de validation simple...")
    
    try:
        await run_simple_performance_tests()
        return True
    except Exception as e:
        print(f"âŒ Erreur lors des tests simples: {e}")
        return False

async def run_full_load_tests(config: LoadTestConfig, output_dir: Path):
    """ExÃ©cute les tests de charge complets"""
    print("\nğŸš€ ExÃ©cution des tests de charge complets...")
    print(f"   â€¢ URL cible: {config.base_url}")
    print(f"   â€¢ Utilisateurs concurrents: {config.concurrent_users}")
    print(f"   â€¢ Total requÃªtes: {config.total_requests}")
    print(f"   â€¢ Limite temps rÃ©ponse: {config.max_response_time_ms}ms")
    print(f"   â€¢ Limite taux erreur: {config.max_error_rate_percent}%")
    print(f"   â€¢ Throughput minimum: {config.min_throughput_rps} req/s")
    
    try:
        # ExÃ©cution des tests complets
        results = await run_comprehensive_load_tests(config)
        
        # Sauvegarde des rÃ©sultats
        csv_file = output_dir / "load_test_results.csv"
        html_file = output_dir / "load_test_report.html"
        json_file = output_dir / "load_test_results.json"
        
        export_results_to_csv(results, str(csv_file))
        generate_html_report(results, str(html_file))
        
        # Sauvegarde JSON pour analyse programmatique
        with open(json_file, 'w', encoding='utf-8') as f:
            json.dump(results, f, indent=2, default=str)
        
        print(f"\nğŸ“„ RÃ©sultats sauvegardÃ©s dans: {output_dir}")
        print(f"   â€¢ Rapport HTML: {html_file.name}")
        print(f"   â€¢ DonnÃ©es CSV: {csv_file.name}")
        print(f"   â€¢ DonnÃ©es JSON: {json_file.name}")
        
        # Affichage du rÃ©sumÃ©
        metrics = results["overall_metrics"]
        print(f"\nğŸ“ˆ RÃ‰SUMÃ‰ DES RÃ‰SULTATS:")
        print(f"   âœ… Total requÃªtes: {metrics['total_requests']}")
        print(f"   âœ… Taux de succÃ¨s: {(metrics['successful_requests']/metrics['total_requests']*100):.1f}%")
        print(f"   âœ… Temps rÃ©ponse moyen: {metrics['avg_response_time_ms']:.0f}ms")
        print(f"   âœ… P95 temps rÃ©ponse: {metrics['p95_response_time_ms']:.0f}ms")
        print(f"   âœ… Throughput: {metrics['requests_per_second']:.1f} req/s")
        print(f"   âœ… Taux d'erreur: {metrics['error_rate_percent']:.2f}%")
        
        # Recommandations
        print(f"\nğŸ’¡ RECOMMANDATIONS:")
        for rec in results["recommendations"]:
            print(f"   {rec}")
        
        # DÃ©termination du succÃ¨s global
        success = (
            metrics['avg_response_time_ms'] <= config.max_response_time_ms and
            metrics['error_rate_percent'] <= config.max_error_rate_percent and
            metrics['requests_per_second'] >= config.min_throughput_rps
        )
        
        if success:
            print(f"\nğŸ‰ TESTS RÃ‰USSIS - L'application est prÃªte pour la production!")
        else:
            print(f"\nâš ï¸ TESTS PARTIELLEMENT RÃ‰USSIS - Optimisations recommandÃ©es")
            
        return success
        
    except Exception as e:
        print(f"âŒ Erreur lors des tests de charge: {e}")
        return False

async def main():
    """Fonction principale"""
    args = parse_arguments()
    
    print_banner()
    
    # PrÃ©paration du rÃ©pertoire de sortie
    output_dir = prepare_output_directory(args.output_dir)
    print(f"ğŸ“ RÃ©pertoire de sortie: {output_dir}")
    
    # Configuration des tests
    config = LoadTestConfig(
        base_url=args.url,
        concurrent_users=args.concurrent_users,
        total_requests=args.total_requests,
        max_response_time_ms=args.max_response_time,
        max_error_rate_percent=args.max_error_rate,
        min_throughput_rps=args.min_throughput
    )
    
    success = True
    
    # 1. Configuration des optimisations
    print("\n" + "="*60)
    print("Ã‰TAPE 1/3: CONFIGURATION DES OPTIMISATIONS")
    print("="*60)
    
    optimization_success = await run_optimization_setup(args.enable_redis)
    if not optimization_success:
        print("âš ï¸ Certaines optimisations ne sont pas disponibles, continuation...")
    
    # 2. Tests selon le mode choisi
    print("\n" + "="*60)
    print("Ã‰TAPE 2/3: EXÃ‰CUTION DES TESTS")
    print("="*60)
    
    if args.mode == 'simple':
        success = await run_simple_tests()
        
    elif args.mode == 'full':
        # Tests simples d'abord
        simple_success = await run_simple_tests()
        
        if simple_success:
            print("\nâœ… Tests simples rÃ©ussis, passage aux tests de charge...")
            success = await run_full_load_tests(config, output_dir)
        else:
            print("\nâŒ Tests simples Ã©chouÃ©s, arrÃªt des tests de charge")
            success = False
            
    elif args.mode == 'optimization-only':
        print("âœ… Configuration des optimisations terminÃ©e")
        
    # 3. Rapport final
    print("\n" + "="*60)
    print("Ã‰TAPE 3/3: RAPPORT FINAL")
    print("="*60)
    
    if success:
        print("ğŸ‰ TOUS LES TESTS SONT TERMINÃ‰S AVEC SUCCÃˆS!")
        if args.mode == 'full':
            print(f"ğŸ“Š Rapports dÃ©taillÃ©s disponibles dans: {output_dir}")
        print("âœ… L'application Ryze est prÃªte pour la production")
        return 0
    else:
        print("âš ï¸ CERTAINS TESTS ONT Ã‰CHOUÃ‰")
        print("ğŸ’¡ VÃ©rifiez les logs et corrigez les problÃ¨mes identifiÃ©s")
        print("ğŸ”§ ConsidÃ©rez l'activation des optimisations Redis avec --enable-redis")
        return 1

if __name__ == "__main__":
    try:
        exit_code = asyncio.run(main())
        sys.exit(exit_code)
    except KeyboardInterrupt:
        print("\nğŸ›‘ Tests interrompus par l'utilisateur")
        sys.exit(130)
    except Exception as e:
        print(f"\nâŒ Erreur fatale: {e}")
        sys.exit(1)